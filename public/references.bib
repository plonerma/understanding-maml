@article{DBLP:journals/corr/AbadiABBCCCDDDG16,
  author    = {Mart{\'{\i}}n Abadi and
               Ashish Agarwal and
               Paul Barham and
               Eugene Brevdo and
               Zhifeng Chen and
               Craig Citro and
               Gregory S. Corrado and
               Andy Davis and
               Jeffrey Dean and
               Matthieu Devin and
               Sanjay Ghemawat and
               Ian J. Goodfellow and
               Andrew Harp and
               Geoffrey Irving and
               Michael Isard and
               Yangqing Jia and
               Rafal J{\'{o}}zefowicz and
               Lukasz Kaiser and
               Manjunath Kudlur and
               Josh Levenberg and
               Dan Man{\'{e}} and
               Rajat Monga and
               Sherry Moore and
               Derek Gordon Murray and
               Chris Olah and
               Mike Schuster and
               Jonathon Shlens and
               Benoit Steiner and
               Ilya Sutskever and
               Kunal Talwar and
               Paul A. Tucker and
               Vincent Vanhoucke and
               Vijay Vasudevan and
               Fernanda B. Vi{\'{e}}gas and
               Oriol Vinyals and
               Pete Warden and
               Martin Wattenberg and
               Martin Wicke and
               Yuan Yu and
               Xiaoqiang Zheng},
  title     = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
               Systems},
  journal   = {CoRR},
  volume    = {abs/1603.04467},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.04467},
  archivePrefix = {arXiv},
  eprint    = {1603.04467},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AbadiABBCCCDDDG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/jmlr/SalakhutdinovTT12,
  author    = {Ruslan Salakhutdinov and
               Joshua B. Tenenbaum and
               Antonio Torralba},
  editor    = {Isabelle Guyon and
               Gideon Dror and
               Vincent Lemaire and
               Graham W. Taylor and
               Daniel L. Silver},
  title     = {One-Shot Learning with a Hierarchical Nonparametric Bayesian Model},
  booktitle = {Unsupervised and Transfer Learning - Workshop held at {ICML} 2011,
               Bellevue, Washington, USA, July 2, 2011},
  series    = {{JMLR} Proceedings},
  volume    = {27},
  pages     = {195--206},
  publisher = {JMLR.org},
  year      = {2012},
  url       = {http://proceedings.mlr.press/v27/salakhutdinov12a/salakhutdinov12a.pdf},
  timestamp = {Wed, 02 Sep 2020 16:33:18 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/SalakhutdinovTT12.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/FinnAL17,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  timestamp = {Thu, 21 Jan 2021 17:37:24 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/FinnAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/cogsci/LakeSGT11,
  author    = {Brenden M. Lake and
               Ruslan Salakhutdinov and
               Jason Gross and
               Joshua B. Tenenbaum},
  editor    = {Laura A. Carlson and
               Christoph H{\"{o}}lscher and
               Thomas F. Shipley},
  title     = {One shot learning of simple visual concepts},
  booktitle = {Proceedings of the 33th Annual Meeting of the Cognitive Science Society,
               CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011},
  publisher = {cognitivesciencesociety.org},
  year      = {2011},
  url       = {https://cogsci.mindmodeling.org/2011/papers/0601/paper0601.pdf},
  timestamp = {Tue, 02 Feb 2021 08:02:11 +0100},
  biburl    = {https://dblp.org/rec/conf/cogsci/LakeSGT11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/nips/VinyalsBLKW16,
  author    = {Oriol Vinyals and
               Charles Blundell and
               Tim Lillicrap and
               Koray Kavukcuoglu and
               Daan Wierstra},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Matching Networks for One Shot Learning},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {3630--3638},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VinyalsBLKW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icann/HochreiterYC01,
  author    = {Sepp Hochreiter and
               A. Steven Younger and
               Peter R. Conwell},
  editor    = {Georg Dorffner and
               Horst Bischof and
               Kurt Hornik},
  title     = {Learning to Learn Using Gradient Descent},
  booktitle = {Artificial Neural Networks - {ICANN} 2001, International Conference
               Vienna, Austria, August 21-25, 2001 Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {2130},
  pages     = {87--94},
  publisher = {Springer},
  year      = {2001},
  url       = {https://doi.org/10.1007/3-540-44668-0\_13},
  doi       = {10.1007/3-540-44668-0\_13},
  timestamp = {Fri, 27 Mar 2020 08:51:30 +0100},
  biburl    = {https://dblp.org/rec/conf/icann/HochreiterYC01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/SantoroBBWL16,
  author    = {Adam Santoro and
               Sergey Bartunov and
               Matthew Botvinick and
               Daan Wierstra and
               Timothy P. Lillicrap},
  editor    = {Maria{-}Florina Balcan and
               Kilian Q. Weinberger},
  title     = {Meta-Learning with Memory-Augmented Neural Networks},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning,
               {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {1842--1850},
  publisher = {JMLR.org},
  year      = {2016},
  url       = {http://proceedings.mlr.press/v48/santoro16.pdf},
  timestamp = {Wed, 29 May 2019 08:41:46 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/SantoroBBWL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/LakeST13,
  author    = {Brenden M. Lake and
               Ruslan Salakhutdinov and
               Joshua B. Tenenbaum},
  editor    = {Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Zoubin Ghahramani and
               Kilian Q. Weinberger},
  title     = {One-shot learning by inverting a compositional causal process},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages     = {2526--2534},
  year      = {2013},
  url       = {https://proceedings.neurips.cc/paper/2013/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf},
  timestamp = {Thu, 21 Jan 2021 15:15:23 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/LakeST13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{koch_siamese_2015,
	title = {Siamese Neural Networks for One-shot Image Recognition},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.},
	language = {en},
    booktitle = "{ICML Workshop
on Deep Learning}",
	author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
	year = {2015},
	url = {https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf},
}


@inproceedings{DBLP:conf/nips/SnellSZ17,
  author    = {Jake Snell and
               Kevin Swersky and
               Richard S. Zemel},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Prototypical Networks for Few-shot Learning},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {4077--4087},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/SnellSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/AndrychowiczDCH16,
  author    = {Marcin Andrychowicz and
               Misha Denil and
               Sergio Gomez Colmenarejo and
               Matthew W. Hoffman and
               David Pfau and
               Tom Schaul and
               Nando de Freitas},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Learning to learn by gradient descent by gradient descent},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {3981--3989},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/file/fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/AndrychowiczDCH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{DBLP:conf/icml/MunkhdalaiY17,
  author    = {Tsendsuren Munkhdalai and
               Hong Yu},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Meta Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {2554--2563},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/munkhdalai17a/munkhdalai17a.pdf},
  timestamp = {Fri, 20 Mar 2020 08:08:25 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/MunkhdalaiY17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{rajeswaran_meta-learning_2019,
	title = {Meta-Learning with Implicit Gradients},
	url = {http://arxiv.org/abs/1909.04630},
	abstract = {A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.},
	urldate = {2021-04-30},
	journal = {arXiv:1909.04630 [cs, math, stat]},
	author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04630},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@article{grant_recasting_2018,
	title = {Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
	url = {http://arxiv.org/abs/1801.08930},
	abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
	urldate = {2021-06-30},
	journal = {arXiv:1801.08930 [cs]},
	author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.08930},
	keywords = {Computer Science - Machine Learning},
}


@article{blondel_efficient_2021,
	title = {Efficient and Modular Implicit Differentiation},
	url = {http://arxiv.org/abs/2105.15183},
	abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a uniﬁed, efﬁcient and modular approach for implicit differentiation of optimization problems. In our approach, the user deﬁnes (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the beneﬁts of implicit differentiation and autodiff. It is efﬁcient as it can be added on top of any state-of-the-art solver and modular as the optimality condition speciﬁcation is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
	language = {en},
	urldate = {2021-07-03},
	journal = {arXiv:2105.15183 [cs, math, stat]},
	author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-López, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
	month = may,
	year = {2021},
	note = {arXiv: 2105.15183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
}

@article{finn_probabilistic_2019,
	title = {Probabilistic Model-Agnostic Meta-Learning},
	url = {http://arxiv.org/abs/1806.02817},
	abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
	urldate = {2021-04-30},
	journal = {arXiv:1806.02817 [cs, stat]},
	author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
	month = oct,
	year = {2019},
	note = {arXiv: 1806.02817
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. First two authors contributed equally. Supplementary results available at https://sites.google.com/view/probabilistic-maml/},
}



@misc{lake_omniglot_2015,
	title = {Omniglot data set for one-shot learning},
	url = {https://github.com/brendenlake/omniglot},
	abstract = {The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.},
	urldate = {2021-07-07},
	journal = {Omniglot data set for one-shot learning},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
	month = oct,
	year = {2015},
}

@misc{liu_tools_2019,
	title = {Tools for mini-ImageNet Dataset},
	journal = {Tools for mini-ImageNet Dataset},
	author = {Liu, Yaoyao},
	month = jan,
	year = {2019},
	url = {https://github.com/yaoyao-liu/mini-imagenet-tools}
}


@article{glorot_deep_nodate,
	title = {Deep Sparse Rectiﬁer Neural Networks},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-diﬀerentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectiﬁer networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the diﬃculty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	pages = {9},
}

@article{sagun_eigenvalues_2017,
	title = {Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond},
	shorttitle = {Eigenvalues of the {Hessian} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.07476},
	abstract = {We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges that depend on the input data.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1611.07476 [cs]},
	aear = {2017},
	note = {arXiv: 1611.07476},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR submission, 2016 - updated to match the openreview.net version},
}

@article{park_meta-curvature_2020,
	title = {Meta-Curvature},
	url = {http://arxiv.org/abs/1902.03356},
	abstract = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the modelagnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model’s parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task speciﬁc techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1902.03356 [cs, stat]},
	author = {Park, Eunbyung and Oliva, Junier B.},
	month = jan,
	year = {2020},
	near = {2017},
	note = {arXiv: 1611.07476},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR submission, 2016 - updated to match the openreview.net version},
}

@article{park_meta-curvature_2020,
	title = {Meta-Curvature},
	url = {http://arxiv.org/abs/1902.03356},
	abstract = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the modelagnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model’s parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task speciﬁc techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1902.03356 [cs, stat]},
	author = {Park, Eunbyung and Oliva, Junier B.},
	month = jan,
	year = {2020},
	note = {arXiv: 1902.03356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in NeurIPS 2019},
}

@article{nichol_first-order_2018,
	title = {On First-Order Meta-Learning Algorithms},
	url = {http://arxiv.org/abs/1803.02999},
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be ﬁne-tuned quickly on a new task, using only ﬁrstorder derivatives for the meta-learning updates. This family includes and generalizes ﬁrst-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that ﬁrst-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classiﬁcation, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1803.02999 [cs]},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.02999},
	keywords = {Computer Science - Machine Learning},
}




@article{nichol_first-order_2018,
	title = {On First-Order Meta-Learning Algorithms},
	url = {http://arxiv.org/abs/1803.02999},
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be ﬁne-tuned quickly on a new task, using only ﬁrstorder derivatives for the meta-learning updates. This family includes and generalizes ﬁrst-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that ﬁrst-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classiﬁcation, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1803.02999 [cs]},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.02999},
	keywords = {Computer Science - Machine Learning},
}
uthor = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	month = oct,
	year = {2017},
	note = {arXiv: 1611.07476},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR submission, 2016 - updated to match the openreview.net version},
}

@article{park_meta-curvature_2020,
	title = {Meta-Curvature},
	url = {http://arxiv.org/abs/1902.03356},
	abstract = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the modelagnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model’s parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task speciﬁc techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1902.03356 [cs, stat]},
	author = {Park, Eunbyung and Oliva, Junier B.},
	month = jan,
	year = {2020},
	note = {arXiv: 1902.03356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in NeurIPS 2019},
}

@article{nichol_first-order_2018,
	title = {On First-Order Meta-Learning Algorithms},
	url = {http://arxiv.org/abs/1803.02999},
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be ﬁne-tuned quickly on a new task, using only ﬁrstorder derivatives for the meta-learning updates. This family includes and generalizes ﬁrst-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that ﬁrst-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classiﬁcation, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1803.02999 [cs]},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.02999},
	keywords = {Computer Science - Machine Learning},
}


@article{liu_darts_2019,
	title = {DARTS: Differentiable Architecture Search},
	shorttitle = {DARTS},
	url = {http://arxiv.org/abs/1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2021-07-22},
	journal = {arXiv:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published at ICLR 2019; Code and pretrained models available at https://github.com/quark0/darts},
}
